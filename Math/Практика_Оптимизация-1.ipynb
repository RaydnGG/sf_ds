{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyHIGVhfa_Wf"
   },
   "source": [
    "# Стохастический градиентный и координатный спуски"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gn6lluIADUKa"
   },
   "source": [
    "Для каждого задания указано количество баллов (если они оцениваются отдельно) + 1 балл за аккуратное и полное выполнение всего задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txCccYvha_Wv"
   },
   "source": [
    "## Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbyOzeZ6a_Wx"
   },
   "source": [
    "**Загрузите уже знакомый вам файл *Advertising.csv* как объект DataFrame.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "E1L4_xeDa_Wz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     TV  radio  newspaper  sales\n",
       "0           1  230.1   37.8       69.2   22.1\n",
       "1           2   44.5   39.3       45.1   10.4\n",
       "2           3   17.2   45.9       69.3    9.3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('Advertising.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf4aVFndDUKf"
   },
   "source": [
    "**Проверьте, есть ли в данных пропуски и, если они есть - удалите их**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tiVeFnR5DUKg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "TV            0\n",
       "radio         0\n",
       "newspaper     0\n",
       "sales         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTkiqPr_DUKh"
   },
   "source": [
    "**Преобразуйте ваши признаки в массивы NumPy и разделите их на переменные X (предикторы) и y(целевая переменная)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "R9OHIRB3a_Xa"
   },
   "outputs": [],
   "source": [
    "# Предикторы\n",
    "X = np.array(data[['TV','radio','newspaper']])\n",
    "# Целевая переменная\n",
    "y = np.array(data['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCvjSoHEDUKo"
   },
   "source": [
    "## Координатный спуск (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjNm8dATDUKq"
   },
   "source": [
    "**Добавим единичный столбец для того, чтобы у нас был свободный коэффициент в уравнении регрессии:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LMgq0fmKDUKr"
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "X = np.hstack([np.ones(X.shape[0]).reshape(-1, 1), X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R008OQwcDUKt"
   },
   "source": [
    "**Нормализуем данные: обычно это необходимо для корректной работы алгоритма**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2Sk7Wx-SDUKt"
   },
   "outputs": [],
   "source": [
    "X = X / np.sqrt(np.sum(np.square(X), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_pHHbAdDUKu"
   },
   "source": [
    "**Реализуйте алгоритм координатного спуска:** (3 балла)\n",
    "\n",
    "Ниже приведен алгоритм:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBQ8vT5UDUKu"
   },
   "source": [
    "<a href=\"https://ibb.co/Th3BQFn\"><img src=\"https://i.ibb.co/DK2DBS6/zascas.jpg\" alt=\"zascas\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ce_yM20DUKv"
   },
   "source": [
    "Примечание: 1000 итераций здесь указаны для этого задания, на самом деле их может быть намного больше, нет детерменированного значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3IdiHm9DUKv"
   },
   "source": [
    "Вам необходимо реализовать координатный спуск, и вывести веса в модели линейной регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Vsi3d9OfDUKw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41.56217205]\n",
      " [110.13144155]\n",
      " [ 73.52860638]\n",
      " [ -0.55006384]]\n"
     ]
    }
   ],
   "source": [
    "y = y.reshape(-1, 1)\n",
    "\n",
    "num_iters = 1000\n",
    "# извлекаем размеры матрицы X\n",
    "m, n = X.shape\n",
    "w = np.zeros((n,1))  # Нулевой вектор весов\n",
    "r = y - X.dot(w)          # Инициализация остатков\n",
    "\n",
    "# Координатный спуск\n",
    "for iteration in range(num_iters):\n",
    "    for j in range(n):\n",
    "        # Вычисляем прогноз без k-ого фактора\n",
    "        h = (X[:,0:j] @ w[0:j]) + (X[:,j+1:] @ w[j+1:])\n",
    "        # Обновляем остатки, удаляя вклад текущего веса w_j\n",
    "        r = y - h\n",
    "        # Обновляем новое значение k-ого коэффициента\n",
    "        w[j] = (X[:,j].T @ (y - h))\n",
    "        # Вычисляем функцию потерь\n",
    "        cost = (sum((X @ w) - y) ** 2)/(len(y))\n",
    "\n",
    "# Выводим вектор весов\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предлагаю для простоты дальнейших расчётов представить вектор весов так:\n",
    "w = w.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3jG-7UADUKx"
   },
   "source": [
    "Сравните результаты с реализацией линейной регрессии из библиотеки sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SBl-1Yb5DUKy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41.56217205 110.13144155  73.52860638  -0.55006384]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    " \n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIl0AGLyDUKy"
   },
   "source": [
    "Если вы все сделали верно, они должны практически совпасть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCGwFnPdDUKz"
   },
   "source": [
    "## Стохастический градиентный спуск (6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5u7Q2YJla_Xk"
   },
   "source": [
    "**Отмасштабируйте столбцы исходной матрицы *X* (которую мы не нормализовали еще!). Для того, чтобы это сделать, надо вычесть из каждого значения среднее и разделить на стандартное отклонение** (0.5 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9cEpV_5La_Xo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее масштабированного массива: 0\n",
      "Стандартные отклонения масштабированного массива: 1\n"
     ]
    }
   ],
   "source": [
    "#Возвращаем изначальную матрицу\n",
    "X = np.array(data[['TV','radio','newspaper']])\n",
    "y = np.array(data['sales'])\n",
    "\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "print('Среднее масштабированного массива: %.0f'%(abs(X.mean())))\n",
    "print('Стандартные отклонения масштабированного массива: %.0f'%(X.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WkNYILHDUK1"
   },
   "source": [
    "**Добавим единичный столбец**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EVl5tEGtDUK1"
   },
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones(X.shape[0]).reshape(-1, 1), X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m53tZA5fDUK1"
   },
   "source": [
    "**Создайте функцию mse_error для вычисления среднеквадратичной ошибки, принимающую два аргумента: реальные значения и предсказывающие, и возвращающую значение mse** (0.5 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "0cvtC08Aa_YK"
   },
   "outputs": [],
   "source": [
    "def mse_error(y, y_pred):\n",
    "    \"\"\"Функция вычисления среднеквадратичной ошибки\n",
    "\n",
    "    Args:\n",
    "        y (float): реальное значение\n",
    "        y_pred (float): предсказанное значение\n",
    "    Returns:\n",
    "        result (float): значение MSE\n",
    "    \"\"\"\n",
    "    mse = ((y - y_pred) ** 2).mean()\n",
    "    return mse # Вычисляем среднее значение реальных продаж\n",
    "\n",
    "\n",
    "mean_sales = np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpOLhdvBDUK2"
   },
   "source": [
    "**Сделайте наивный прогноз: предскажите продажи средним значением. После этого рассчитайте среднеквадратичную ошибку для этого прогноза** (0.5 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "kLV_XljVa_YZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наивный прогноз (среднее значение продаж): 14.0225\n",
      "Среднеквадратичная ошибка (MSE) для наивного прогноза: 27.085743750000002\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.full_like(y, mean_sales) \n",
    "# расчет MSE\n",
    "mse = mse_error(y, y_pred)\n",
    "\n",
    "print(\"Наивный прогноз (среднее значение продаж):\", y_pred[0])\n",
    "print(\"Среднеквадратичная ошибка (MSE) для наивного прогноза:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbybL2ola_ZM"
   },
   "source": [
    "**Создайте функцию *lin_pred*, которая может по матрице предикторов *X* и вектору весов линейной модели *w* получить вектор прогнозов** (0.5 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "1Cyz-Luaa_ZO"
   },
   "outputs": [],
   "source": [
    "def lin_pred(X, w):\n",
    "    \"\"\"Функция получит предсказания по весам линейной модели\n",
    "\n",
    "    Args:\n",
    "        X (array): матрица предикторов\n",
    "        w (array): вектор весов линейной модели\n",
    "\n",
    "    Returns:\n",
    "        array: вектор прогнозов\n",
    "    \"\"\"\n",
    "    y_pred = X@w\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.7841263145109365\n"
     ]
    }
   ],
   "source": [
    "# Мы произвели стандартизацию вектора Х, поэтому нам нужно обновить вектор весов w\n",
    "# Обновим веса w для отмасштабированных данных\n",
    "def lin_reg(X, y):\n",
    "    a = np.dot(X.T, X)\n",
    "    b = np.dot(X.T, y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "\n",
    "w = lin_reg(X, y).T\n",
    "\n",
    "y_pred = lin_pred(X, w)\n",
    "print(f'MSE: {mse_error(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU4adBrya_Zm"
   },
   "source": [
    "**Создайте функцию *stoch_grad_step* для реализации шага стохастического градиентного спуска. (1.5 балла) \n",
    "Функция должна принимать на вход следующие аргументы:**\n",
    "* матрицу *X*\n",
    "* вектора *y* и *w*\n",
    "* число *train_ind* - индекс объекта обучающей выборки (строки матрицы *X*), по которому считается изменение весов\n",
    "* число *$\\eta$* (eta) - шаг градиентного спуска\n",
    "\n",
    "Результатом будет вектор обновленных весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyLY-P02DUK5"
   },
   "source": [
    "Шаг для стохастического градиентного спуска выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORsAyIKNDUK5"
   },
   "source": [
    "$$\\Large w_j \\leftarrow w_j - \\frac{2\\eta}{\\ell} \\sum_{i=1}^\\ell{{x_{ij}((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQl2FrpuDUK6"
   },
   "source": [
    "Для того, чтобы написать функцию, нужно сделать следующее:\n",
    "    \n",
    "*  посчитать направление изменения: умножить объект обучающей выборки на 2 и на разницу между предсказанным значением и реальным, а потом поделить на количество элементов в выборке.\n",
    "* вернуть разницу между вектором весов и направлением изменения, умноженным на шаг градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YUhVQGsja_Zn"
   },
   "outputs": [],
   "source": [
    "def stoch_grad_step(X, y, w, index, eta=0.01):\n",
    "    \"\"\"summary\n",
    "\n",
    "    Args:\n",
    "        X (array): матрица предикторов\n",
    "        y (array): вектор ответов\n",
    "        w (array): вектор весов\n",
    "        train_ind (int): индекс объекта\n",
    "        eta (float): шаг градиентного спуска\n",
    "    Returns:\n",
    "        array: обновленный вектор весов\n",
    "    \"\"\"\n",
    "    x_sample = X[index]\n",
    "    y_sample = y[index]\n",
    "    \n",
    "    y_pred = x_sample @ w\n",
    "    \n",
    "    gradient = x_sample * (y_pred-y_sample) / len(X)\n",
    "    weights = w - 2 * eta * gradient\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.02261487,  3.91934441,  2.79206845, -0.02267902])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Опять же желательно проверять работоспособность функций\n",
    "stoch_grad_step(X, y, w, 11, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXwIFd0Ma_Zx"
   },
   "source": [
    "**Создайте функцию *stochastic_gradient_descent*, для реализации стохастического градиентного спуска (2.5 балла)**\n",
    "\n",
    "**Функция принимает на вход следующие аргументы:**\n",
    "- Матрицу признаков X\n",
    "- Целевую переменнную\n",
    "- Изначальную точку (веса модели)\n",
    "- Параметр, определяющий темп обучения\n",
    "- Максимальное число итераций\n",
    "- Евклидово расстояние между векторами весов на соседних итерациях градиентного спуска,при котором алгоритм прекращает работу \n",
    "\n",
    "**На каждой итерации в вектор (список) должно записываться текущее значение среднеквадратичной ошибки. Функция должна возвращать вектор весов $w$, а также вектор (список) ошибок.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVeoNF1JDUK7"
   },
   "source": [
    "Алгоритм сследующий:\n",
    "    \n",
    "* Инициализируйте расстояние между векторами весов на соседних итерациях большим числом (можно бесконечностью)\n",
    "* Создайте пустой список для фиксации ошибок\n",
    "* Создайте счетчик итераций\n",
    "* Реализуйте оновной цикл обучения пока расстояние между векторами весов больше того, при котором надо прекратить работу (когда расстояния станут слишком маленькими - значит, мы застряли в одном месте) и количество итераций меньше максимально разрешенного: сгенерируйте случайный индекс, запишите текущую ошибку в вектор ошибок, запишите в переменную текущий шаг стохастического спуска с использованием функции, написанной ранее. Далее рассчитайте текущее расстояние между векторами весов и прибавьте к счетчику итераций 1.\n",
    "* Верните вектор весов и вектор ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "CD_xcFNfa_Zy"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, eta=0.1, max_iter=1e+4, dist_min=1e-8):\n",
    "    \"\"\"Функция реализующая стохастический градиентный спуск\n",
    "    \n",
    "    Args:\n",
    "        X (array): матрица предикторов\n",
    "        y (array): вектор ответов\n",
    "        w (array): вектор весов\n",
    "        eta (float): шаг градиентного спуска\n",
    "        max_iter (type): максимальное количество итерации\n",
    "        dist_min (type): минимальное расстояние между векторами весов\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 1e+10 #расстояние между векторами\n",
    "    errors = [] #список для фиксации ошибок\n",
    "    iters = 0\n",
    "\n",
    "    w_value = []\n",
    "    \n",
    "    while distance > dist_min and iters < max_iter:\n",
    "        random_ind = np.random.randint(X.shape[0])\n",
    "        y_pred = lin_pred(X, w)\n",
    "        \n",
    "        errors.append(mse_error(y, y_pred))\n",
    "        w_new = stoch_grad_step(X, y, w, random_ind, eta)\n",
    "        \n",
    "        distance = np.linalg.norm(w - w_new)\n",
    "        w_value.append(w)\n",
    "        \n",
    "        w = w_new\n",
    "        iters += 1\n",
    "        \n",
    "    return w, w_value, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OqHO1Rta_Z7"
   },
   "source": [
    " **Запустите $10^5$ итераций стохастического градиентного спуска. Укажите вектор начальных весов, состоящий из нулей. Можете поэкспериментировать с параметром, отвечающим за темп обучения.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6fHHT6vDUK8"
   },
   "source": [
    "**Постройте график зависимости ошибки от номера итерации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "zsSfHDzLDUK9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x274a52a8710>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, w_values, errors = stochastic_gradient_descent(X, y, np.zeros(X.shape[1]), 2, 1e+5, dist_min=1e-3)\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-MVOcJ6a_aY"
   },
   "source": [
    "**Выведите вектор весов, к которому сошелся метод.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "MPjVkXe4DUK9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор весов, к которому сошелся метод: [14.02030767  4.02930288  2.56319722 -0.06124374]\n"
     ]
    }
   ],
   "source": [
    "print(f'Вектор весов, к которому сошелся метод: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qabzMc3Qa_a5"
   },
   "source": [
    "**Выведите среднеквадратичную ошибку на последней итерации.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7tPWleMIa_a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8531495074587347"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_error(X@w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Минимальная целевая метрика достигнута на       207 итерации, и составила       2.7932639436798974       \n",
      "Веса линейной регрессии при этом составили [13.97783471  3.94844051  2.75546278  0.06085254]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_16888\\2623653776.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  w_optimal = model_data.iloc[model_data['MSE'].idxmin()][1]\n"
     ]
    }
   ],
   "source": [
    "# Для поиска итерации на которой достигнут минимум целевой метрики используем Pandas\n",
    "model_data = pd.DataFrame({'MSE': errors, 'w_value': w_values})\n",
    "w_optimal = model_data.iloc[model_data['MSE'].idxmin()][1]\n",
    "\n",
    "print(f'Минимальная целевая метрика достигнута на \\\n",
    "      {model_data[\"MSE\"].idxmin()} итерации, и составила \\\n",
    "      {errors[model_data[\"MSE\"].idxmin()]} \\\n",
    "      \\nВеса линейной регрессии при этом составили {w_optimal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "если проще - градиентный спуск с учётом предыдущего шага\n",
    "***\n",
    "Формально можно записать, что для того, чтобы попасть в следующую точку $x_1$, необходимо перейти из начальной точки $x_0$ на антиградиент, домноженный на некоторый коэффициент, который называется **шагом градиентного спуска** или **темпом обучения** — о нём мы поговорим немного позже.\n",
    "\n",
    "$x_1 = x_0 - \\alpha \\nabla f (x_0)$\n",
    "\n",
    "$\\alpha$ - темп обучения\n",
    "\n",
    "$\\nabla f = \\left ( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right )$ - вектор градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим функцию\n",
    "def fun(x, y, a=1, b=1):\n",
    "    return a * (x**2) + b * (y**2)\n",
    "\n",
    "\n",
    "# определим градиент\n",
    "def grad(x, y, a=1, b=1):\n",
    "    return np.array([2 * a * x, 2 * b * y])\n",
    "\n",
    "\n",
    "# простой градиентный спуск\n",
    "def grad_descend(grad, step_size=0.2, num_steps=30):\n",
    "    lst = []\n",
    "    x = np.random.uniform(0, 3, size = 2)\n",
    "    lst.append(x)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        x = x - step_size * grad(lst[-1][0], lst[-1][1])\n",
    "        lst.append(x)\n",
    "        \n",
    "    return np.array(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск с `momentum`. Формально его можно определить следующим образом:\n",
    "\n",
    "$x_{n+1}=x_n - \\alpha\\nabla f(x_n) + \\gamma(x_n - x_{n-1})$\n",
    "\n",
    "В формуле выше $\\gamma$ — это параметр, который показывает, насколько учитывается предыдущий шаг.\n",
    "\n",
    "Для примера найдём минимум функции $2x^2 - 4xy + y^4 + 2$ с помощью градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1; x=(0.0800, 0.9200); f(x)=2.4348;           grad f(x)=(-3.3600, 2.7948)\n",
      "iter=2; x=(0.1472, 0.8641); f(x)=2.0921;           grad f(x)=(-2.8676, 1.9920)\n",
      "iter=3; x=(0.2046, 0.8243); f(x)=1.8709;           grad f(x)=(-2.4788, 1.4218)\n",
      "iter=4; x=(0.2541, 0.7958); f(x)=1.7213;           grad f(x)=(-2.1668, 0.9996)\n",
      "iter=5; x=(0.2975, 0.7758); f(x)=1.6161;           grad f(x)=(-1.9135, 0.6781)\n",
      "iter=6; x=(0.3357, 0.7623); f(x)=1.5394;           grad f(x)=(-1.7062, 0.4288)\n",
      "iter=7; x=(0.3699, 0.7537); f(x)=1.4812;           grad f(x)=(-1.5354, 0.2332)\n",
      "iter=8; x=(0.4006, 0.7490); f(x)=1.4355;           grad f(x)=(-1.3939, 0.0787)\n",
      "iter=9; x=(0.4284, 0.7475); f(x)=1.3983;           grad f(x)=(-1.2761, -0.0434)\n",
      "iter=10; x=(0.4540, 0.7483); f(x)=1.3669;           grad f(x)=(-1.1775, -0.1396)\n",
      "iter=11; x=(0.4775, 0.7511); f(x)=1.3397;           grad f(x)=(-1.0944, -0.2150)\n",
      "iter=12; x=(0.4994, 0.7554); f(x)=1.3154;           grad f(x)=(-1.0241, -0.2733)\n",
      "iter=13; x=(0.5199, 0.7609); f(x)=1.2935;           grad f(x)=(-0.9640, -0.3175)\n",
      "iter=14; x=(0.5392, 0.7672); f(x)=1.2732;           grad f(x)=(-0.9123, -0.3501)\n",
      "iter=15; x=(0.5574, 0.7742); f(x)=1.2545;           grad f(x)=(-0.8673, -0.3732)\n",
      "iter=16; x=(0.5748, 0.7817); f(x)=1.2369;           grad f(x)=(-0.8278, -0.3884)\n",
      "iter=17; x=(0.5913, 0.7895); f(x)=1.2205;           grad f(x)=(-0.7926, -0.3971)\n",
      "iter=18; x=(0.6072, 0.7974); f(x)=1.2050;           grad f(x)=(-0.7610, -0.4005)\n",
      "iter=19; x=(0.6224, 0.8054); f(x)=1.1904;           grad f(x)=(-0.7321, -0.3996)\n",
      "iter=20; x=(0.6370, 0.8134); f(x)=1.1767;           grad f(x)=(-0.7055, -0.3954)\n",
      "iter=21; x=(0.6511, 0.8213); f(x)=1.1638;           grad f(x)=(-0.6807, -0.3884)\n",
      "iter=22; x=(0.6648, 0.8291); f(x)=1.1517;           grad f(x)=(-0.6573, -0.3794)\n",
      "iter=23; x=(0.6779, 0.8367); f(x)=1.1404;           grad f(x)=(-0.6351, -0.3688)\n",
      "iter=24; x=(0.6906, 0.8441); f(x)=1.1298;           grad f(x)=(-0.6138, -0.3571)\n",
      "iter=25; x=(0.7029, 0.8512); f(x)=1.1199;           grad f(x)=(-0.5933, -0.3446)\n",
      "iter=26; x=(0.7147, 0.8581); f(x)=1.1106;           grad f(x)=(-0.5734, -0.3317)\n",
      "iter=27; x=(0.7262, 0.8647); f(x)=1.1020;           grad f(x)=(-0.5540, -0.3185)\n",
      "iter=28; x=(0.7373, 0.8711); f(x)=1.0940;           grad f(x)=(-0.5352, -0.3052)\n",
      "iter=29; x=(0.7480, 0.8772); f(x)=1.0865;           grad f(x)=(-0.5168, -0.2921)\n",
      "iter=30; x=(0.7583, 0.8830); f(x)=1.0796;           grad f(x)=(-0.4988, -0.2791)\n",
      "iter=31; x=(0.7683, 0.8886); f(x)=1.0732;           grad f(x)=(-0.4812, -0.2665)\n",
      "iter=32; x=(0.7779, 0.8939); f(x)=1.0673;           grad f(x)=(-0.4641, -0.2542)\n",
      "iter=33; x=(0.7872, 0.8990); f(x)=1.0618;           grad f(x)=(-0.4473, -0.2423)\n",
      "iter=34; x=(0.7962, 0.9039); f(x)=1.0567;           grad f(x)=(-0.4309, -0.2308)\n",
      "iter=35; x=(0.8048, 0.9085); f(x)=1.0520;           grad f(x)=(-0.4149, -0.2198)\n",
      "iter=36; x=(0.8131, 0.9129); f(x)=1.0477;           grad f(x)=(-0.3993, -0.2092)\n",
      "iter=37; x=(0.8211, 0.9171); f(x)=1.0437;           grad f(x)=(-0.3841, -0.1991)\n",
      "iter=38; x=(0.8287, 0.9211); f(x)=1.0400;           grad f(x)=(-0.3693, -0.1895)\n",
      "iter=39; x=(0.8361, 0.9248); f(x)=1.0367;           grad f(x)=(-0.3549, -0.1803)\n",
      "iter=40; x=(0.8432, 0.9285); f(x)=1.0336;           grad f(x)=(-0.3409, -0.1715)\n",
      "iter=41; x=(0.8500, 0.9319); f(x)=1.0307;           grad f(x)=(-0.3274, -0.1632)\n",
      "iter=42; x=(0.8566, 0.9351); f(x)=1.0281;           grad f(x)=(-0.3142, -0.1552)\n",
      "iter=43; x=(0.8629, 0.9382); f(x)=1.0257;           grad f(x)=(-0.3015, -0.1477)\n",
      "iter=44; x=(0.8689, 0.9412); f(x)=1.0235;           grad f(x)=(-0.2892, -0.1405)\n",
      "iter=45; x=(0.8747, 0.9440); f(x)=1.0215;           grad f(x)=(-0.2773, -0.1337)\n",
      "iter=46; x=(0.8802, 0.9467); f(x)=1.0196;           grad f(x)=(-0.2658, -0.1272)\n",
      "iter=47; x=(0.8855, 0.9492); f(x)=1.0179;           grad f(x)=(-0.2547, -0.1210)\n",
      "iter=48; x=(0.8906, 0.9517); f(x)=1.0163;           grad f(x)=(-0.2440, -0.1152)\n",
      "iter=49; x=(0.8955, 0.9540); f(x)=1.0149;           grad f(x)=(-0.2337, -0.1096)\n",
      "iter=50; x=(0.9002, 0.9561); f(x)=1.0136;           grad f(x)=(-0.2238, -0.1043)\n",
      "iter=51; x=(0.9047, 0.9582); f(x)=1.0124;           grad f(x)=(-0.2142, -0.0993)\n",
      "iter=52; x=(0.9090, 0.9602); f(x)=1.0113;           grad f(x)=(-0.2050, -0.0945)\n",
      "iter=53; x=(0.9131, 0.9621); f(x)=1.0103;           grad f(x)=(-0.1962, -0.0899)\n",
      "iter=54; x=(0.9170, 0.9639); f(x)=1.0094;           grad f(x)=(-0.1877, -0.0856)\n",
      "iter=55; x=(0.9207, 0.9656); f(x)=1.0086;           grad f(x)=(-0.1795, -0.0815)\n",
      "iter=56; x=(0.9243, 0.9672); f(x)=1.0078;           grad f(x)=(-0.1717, -0.0776)\n",
      "iter=57; x=(0.9278, 0.9688); f(x)=1.0071;           grad f(x)=(-0.1642, -0.0739)\n",
      "iter=58; x=(0.9310, 0.9703); f(x)=1.0065;           grad f(x)=(-0.1569, -0.0703)\n",
      "iter=59; x=(0.9342, 0.9717); f(x)=1.0059;           grad f(x)=(-0.1500, -0.0670)\n",
      "iter=60; x=(0.9372, 0.9730); f(x)=1.0054;           grad f(x)=(-0.1434, -0.0638)\n",
      "iter=61; x=(0.9401, 0.9743); f(x)=1.0049;           grad f(x)=(-0.1370, -0.0607)\n",
      "iter=62; x=(0.9428, 0.9755); f(x)=1.0045;           grad f(x)=(-0.1309, -0.0578)\n",
      "iter=63; x=(0.9454, 0.9767); f(x)=1.0041;           grad f(x)=(-0.1251, -0.0551)\n",
      "iter=64; x=(0.9479, 0.9778); f(x)=1.0037;           grad f(x)=(-0.1195, -0.0525)\n",
      "iter=65; x=(0.9503, 0.9788); f(x)=1.0034;           grad f(x)=(-0.1141, -0.0500)\n",
      "iter=66; x=(0.9526, 0.9798); f(x)=1.0031;           grad f(x)=(-0.1090, -0.0476)\n",
      "iter=67; x=(0.9548, 0.9808); f(x)=1.0028;           grad f(x)=(-0.1041, -0.0453)\n",
      "iter=68; x=(0.9568, 0.9817); f(x)=1.0026;           grad f(x)=(-0.0994, -0.0432)\n",
      "iter=69; x=(0.9588, 0.9825); f(x)=1.0023;           grad f(x)=(-0.0949, -0.0411)\n",
      "iter=70; x=(0.9607, 0.9834); f(x)=1.0021;           grad f(x)=(-0.0906, -0.0392)\n",
      "iter=71; x=(0.9625, 0.9842); f(x)=1.0019;           grad f(x)=(-0.0865, -0.0373)\n",
      "iter=72; x=(0.9643, 0.9849); f(x)=1.0017;           grad f(x)=(-0.0825, -0.0356)\n",
      "iter=73; x=(0.9659, 0.9856); f(x)=1.0016;           grad f(x)=(-0.0788, -0.0339)\n",
      "iter=74; x=(0.9675, 0.9863); f(x)=1.0014;           grad f(x)=(-0.0752, -0.0323)\n",
      "iter=75; x=(0.9690, 0.9869); f(x)=1.0013;           grad f(x)=(-0.0717, -0.0308)\n",
      "iter=76; x=(0.9704, 0.9875); f(x)=1.0012;           grad f(x)=(-0.0685, -0.0293)\n",
      "iter=77; x=(0.9718, 0.9881); f(x)=1.0011;           grad f(x)=(-0.0653, -0.0279)\n",
      "iter=78; x=(0.9731, 0.9887); f(x)=1.0010;           grad f(x)=(-0.0623, -0.0266)\n",
      "iter=79; x=(0.9744, 0.9892); f(x)=1.0009;           grad f(x)=(-0.0595, -0.0253)\n",
      "iter=80; x=(0.9755, 0.9897); f(x)=1.0008;           grad f(x)=(-0.0567, -0.0241)\n",
      "iter=81; x=(0.9767, 0.9902); f(x)=1.0007;           grad f(x)=(-0.0541, -0.0230)\n",
      "iter=82; x=(0.9778, 0.9907); f(x)=1.0007;           grad f(x)=(-0.0517, -0.0219)\n",
      "iter=83; x=(0.9788, 0.9911); f(x)=1.0006;           grad f(x)=(-0.0493, -0.0209)\n",
      "iter=84; x=(0.9798, 0.9915); f(x)=1.0006;           grad f(x)=(-0.0470, -0.0199)\n",
      "iter=85; x=(0.9807, 0.9919); f(x)=1.0005;           grad f(x)=(-0.0448, -0.0190)\n",
      "iter=86; x=(0.9816, 0.9923); f(x)=1.0005;           grad f(x)=(-0.0428, -0.0181)\n",
      "iter=87; x=(0.9825, 0.9927); f(x)=1.0004;           grad f(x)=(-0.0408, -0.0172)\n",
      "iter=88; x=(0.9833, 0.9930); f(x)=1.0004;           grad f(x)=(-0.0389, -0.0164)\n",
      "iter=89; x=(0.9841, 0.9933); f(x)=1.0003;           grad f(x)=(-0.0371, -0.0156)\n",
      "iter=90; x=(0.9848, 0.9937); f(x)=1.0003;           grad f(x)=(-0.0354, -0.0149)\n",
      "iter=91; x=(0.9855, 0.9940); f(x)=1.0003;           grad f(x)=(-0.0337, -0.0142)\n",
      "iter=92; x=(0.9862, 0.9942); f(x)=1.0003;           grad f(x)=(-0.0322, -0.0135)\n",
      "iter=93; x=(0.9868, 0.9945); f(x)=1.0002;           grad f(x)=(-0.0307, -0.0129)\n",
      "iter=94; x=(0.9874, 0.9948); f(x)=1.0002;           grad f(x)=(-0.0293, -0.0123)\n",
      "iter=95; x=(0.9880, 0.9950); f(x)=1.0002;           grad f(x)=(-0.0279, -0.0117)\n",
      "iter=96; x=(0.9886, 0.9952); f(x)=1.0002;           grad f(x)=(-0.0266, -0.0112)\n",
      "iter=97; x=(0.9891, 0.9955); f(x)=1.0002;           grad f(x)=(-0.0254, -0.0106)\n",
      "iter=98; x=(0.9896, 0.9957); f(x)=1.0001;           grad f(x)=(-0.0242, -0.0101)\n",
      "iter=99; x=(0.9901, 0.9959); f(x)=1.0001;           grad f(x)=(-0.0231, -0.0097)\n",
      "iter=100; x=(0.9906, 0.9961); f(x)=1.0001;           grad f(x)=(-0.0220, -0.0092)\n",
      "iter=101; x=(0.9910, 0.9963); f(x)=1.0001;           grad f(x)=(-0.0210, -0.0088)\n",
      "iter=102; x=(0.9914, 0.9964); f(x)=1.0001;           grad f(x)=(-0.0200, -0.0084)\n",
      "iter=103; x=(0.9918, 0.9966); f(x)=1.0001;           grad f(x)=(-0.0191, -0.0080)\n",
      "iter=104; x=(0.9922, 0.9968); f(x)=1.0001;           grad f(x)=(-0.0182, -0.0076)\n",
      "iter=105; x=(0.9926, 0.9969); f(x)=1.0001;           grad f(x)=(-0.0173, -0.0072)\n",
      "iter=106; x=(0.9929, 0.9971); f(x)=1.0001;           grad f(x)=(-0.0165, -0.0069)\n",
      "iter=107; x=(0.9933, 0.9972); f(x)=1.0001;           grad f(x)=(-0.0158, -0.0066)\n",
      "iter=108; x=(0.9936, 0.9973); f(x)=1.0001;           grad f(x)=(-0.0150, -0.0063)\n",
      "iter=109; x=(0.9939, 0.9975); f(x)=1.0001;           grad f(x)=(-0.0143, -0.0060)\n",
      "iter=110; x=(0.9942, 0.9976); f(x)=1.0000;           grad f(x)=(-0.0137, -0.0057)\n",
      "iter=111; x=(0.9944, 0.9977); f(x)=1.0000;           grad f(x)=(-0.0130, -0.0054)\n",
      "iter=112; x=(0.9947, 0.9978); f(x)=1.0000;           grad f(x)=(-0.0124, -0.0052)\n",
      "iter=113; x=(0.9949, 0.9979); f(x)=1.0000;           grad f(x)=(-0.0118, -0.0049)\n",
      "iter=114; x=(0.9952, 0.9980); f(x)=1.0000;           grad f(x)=(-0.0113, -0.0047)\n",
      "iter=115; x=(0.9954, 0.9981); f(x)=1.0000;           grad f(x)=(-0.0108, -0.0045)\n",
      "iter=116; x=(0.9956, 0.9982); f(x)=1.0000;           grad f(x)=(-0.0102, -0.0043)\n",
      "iter=117; x=(0.9958, 0.9983); f(x)=1.0000;           grad f(x)=(-0.0098, -0.0041)\n",
      "iter=118; x=(0.9960, 0.9983); f(x)=1.0000;           grad f(x)=(-0.0093, -0.0039)\n",
      "iter=119; x=(0.9962, 0.9984); f(x)=1.0000;           grad f(x)=(-0.0089, -0.0037)\n",
      "iter=120; x=(0.9964, 0.9985); f(x)=1.0000;           grad f(x)=(-0.0085, -0.0035)\n",
      "iter=121; x=(0.9966, 0.9986); f(x)=1.0000;           grad f(x)=(-0.0081, -0.0034)\n",
      "iter=122; x=(0.9967, 0.9986); f(x)=1.0000;           grad f(x)=(-0.0077, -0.0032)\n",
      "iter=123; x=(0.9969, 0.9987); f(x)=1.0000;           grad f(x)=(-0.0073, -0.0030)\n",
      "iter=124; x=(0.9970, 0.9988); f(x)=1.0000;           grad f(x)=(-0.0070, -0.0029)\n",
      "iter=125; x=(0.9972, 0.9988); f(x)=1.0000;           grad f(x)=(-0.0067, -0.0028)\n",
      "iter=126; x=(0.9973, 0.9989); f(x)=1.0000;           grad f(x)=(-0.0064, -0.0026)\n",
      "iter=127; x=(0.9974, 0.9989); f(x)=1.0000;           grad f(x)=(-0.0061, -0.0025)\n",
      "iter=128; x=(0.9975, 0.9990); f(x)=1.0000;           grad f(x)=(-0.0058, -0.0024)\n",
      "iter=129; x=(0.9977, 0.9990); f(x)=1.0000;           grad f(x)=(-0.0055, -0.0023)\n",
      "iter=130; x=(0.9978, 0.9991); f(x)=1.0000;           grad f(x)=(-0.0052, -0.0022)\n",
      "iter=131; x=(0.9979, 0.9991); f(x)=1.0000;           grad f(x)=(-0.0050, -0.0021)\n",
      "iter=132; x=(0.9980, 0.9992); f(x)=1.0000;           grad f(x)=(-0.0048, -0.0020)\n",
      "iter=133; x=(0.9981, 0.9992); f(x)=1.0000;           grad f(x)=(-0.0045, -0.0019)\n",
      "iter=134; x=(0.9982, 0.9992); f(x)=1.0000;           grad f(x)=(-0.0043, -0.0018)\n",
      "iter=135; x=(0.9982, 0.9993); f(x)=1.0000;           grad f(x)=(-0.0041, -0.0017)\n",
      "iter=136; x=(0.9983, 0.9993); f(x)=1.0000;           grad f(x)=(-0.0039, -0.0016)\n",
      "iter=137; x=(0.9984, 0.9993); f(x)=1.0000;           grad f(x)=(-0.0037, -0.0016)\n",
      "iter=138; x=(0.9985, 0.9994); f(x)=1.0000;           grad f(x)=(-0.0036, -0.0015)\n",
      "iter=139; x=(0.9985, 0.9994); f(x)=1.0000;           grad f(x)=(-0.0034, -0.0014)\n",
      "iter=140; x=(0.9986, 0.9994); f(x)=1.0000;           grad f(x)=(-0.0032, -0.0013)\n",
      "iter=141; x=(0.9987, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0031, -0.0013)\n",
      "iter=142; x=(0.9987, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0029, -0.0012)\n",
      "iter=143; x=(0.9988, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0028, -0.0012)\n",
      "iter=144; x=(0.9989, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0027, -0.0011)\n",
      "iter=145; x=(0.9989, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0026, -0.0011)\n",
      "iter=146; x=(0.9990, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0024, -0.0010)\n",
      "iter=147; x=(0.9990, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0023, -0.0010)\n",
      "iter=148; x=(0.9991, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0022, -0.0009)\n",
      "iter=149; x=(0.9991, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0021, -0.0009)\n",
      "iter=150; x=(0.9991, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0020, -0.0008)\n",
      "iter=151; x=(0.9992, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0019, -0.0008)\n",
      "iter=152; x=(0.9992, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0018, -0.0008)\n",
      "iter=153; x=(0.9993, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0017, -0.0007)\n",
      "iter=154; x=(0.9993, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0017, -0.0007)\n",
      "iter=155; x=(0.9993, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0016, -0.0007)\n",
      "iter=156; x=(0.9994, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0015, -0.0006)\n",
      "iter=157; x=(0.9994, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0014, -0.0006)\n",
      "iter=158; x=(0.9994, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0014, -0.0006)\n",
      "iter=159; x=(0.9994, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0013, -0.0005)\n",
      "iter=160; x=(0.9995, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0012, -0.0005)\n",
      "iter=161; x=(0.9995, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0012, -0.0005)\n",
      "iter=162; x=(0.9995, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0011, -0.0005)\n",
      "iter=163; x=(0.9995, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0011, -0.0004)\n",
      "iter=164; x=(0.9996, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0010, -0.0004)\n",
      "iter=165; x=(0.9996, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0010, -0.0004)\n",
      "iter=166; x=(0.9996, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0009, -0.0004)\n",
      "iter=167; x=(0.9996, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0009, -0.0004)\n",
      "iter=168; x=(0.9996, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0008, -0.0004)\n",
      "iter=169; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0008, -0.0003)\n",
      "iter=170; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0008, -0.0003)\n",
      "iter=171; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0007, -0.0003)\n",
      "iter=172; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0007, -0.0003)\n",
      "iter=173; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0007, -0.0003)\n",
      "iter=174; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0006, -0.0003)\n",
      "iter=175; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0006, -0.0003)\n",
      "iter=176; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0006, -0.0002)\n",
      "iter=177; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0006, -0.0002)\n",
      "iter=178; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0005, -0.0002)\n",
      "iter=179; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0005, -0.0002)\n",
      "iter=180; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0005, -0.0002)\n",
      "iter=181; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0005, -0.0002)\n",
      "iter=182; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0002)\n",
      "iter=183; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0002)\n",
      "iter=184; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0002)\n",
      "iter=185; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0002)\n",
      "iter=186; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0001)\n",
      "iter=187; x=(0.9999, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
      "iter=188; x=(0.9999, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
      "iter=189; x=(0.9999, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
      "iter=190; x=(0.9999, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
      "iter=191; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
      "iter=192; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
      "iter=193; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
      "iter=194; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=195; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=196; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=197; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=198; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=199; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=200; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=201; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=202; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=203; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=204; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
      "iter=205; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0001)\n",
      "iter=206; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0001)\n",
      "iter=207; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0001)\n",
      "iter=208; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0001)\n",
      "iter=209; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=210; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=211; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=212; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=213; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=214; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=215; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=216; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=217; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=218; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=219; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=220; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=221; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=222; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=223; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=224; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=225; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=226; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
      "iter=227; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=228; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=229; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=230; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=231; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=232; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=233; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=234; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=235; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=236; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=237; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=238; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=239; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=240; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=241; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=242; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=243; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=244; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=245; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=246; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=247; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=248; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=249; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=250; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=251; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=252; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=253; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=254; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=255; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=256; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=257; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=258; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=259; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=260; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=261; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=262; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=263; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=264; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=265; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=266; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=267; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=268; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=269; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=270; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=271; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=272; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=273; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=274; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=275; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=276; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=277; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=278; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=279; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=280; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=281; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=282; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=283; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=284; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=285; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=286; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=287; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=288; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=289; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=290; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=291; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=292; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=293; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=294; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=295; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=296; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=297; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=298; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=299; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=300; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
      "iter=301; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n"
     ]
    }
   ],
   "source": [
    "def f(x, y): # сама функция далее не вызывается\n",
    "    return 2*x**2-4*x*y + y**4 + 2 # но объявим её\n",
    "\n",
    "\n",
    "def grad(x, y): # вычислятель градиента\n",
    "    dx = 4*x-4*y # частная производная по х\n",
    "    dy = 4*y**3-4*x # частная производная по у\n",
    "    return (dx, dy)\n",
    "\n",
    "\n",
    "x0 = (0, 1) # начальная точка\n",
    "gamma = 0.02 # темп обучения\n",
    "x_cur = x0 # текущая точка (на 1ой итерации совпадает с начальной)\n",
    "\n",
    "vals = []\n",
    "coords = []\n",
    "i = 0\n",
    "\n",
    "while True: # запускаем бесконечный цикл\n",
    "    # метод градиентного спуска посчитанный покординатно\n",
    "    x_new = (x_cur[0] - gamma * grad(*x_cur)[0],\n",
    "            x_cur[1] - gamma * grad(*x_cur)[1]) \n",
    "    \n",
    "    if i > 300: # зададим конечное количество итераций\n",
    "        break\n",
    "        \n",
    "    x_cur = x_new # перезапишем текущую точку\n",
    "    vals.append(f(*x_cur))\n",
    "    coords.append(x_cur)\n",
    "    i += 1 # добавим счётчик итераций\n",
    "\n",
    "    # выведем интересующие нас значения\n",
    "    print(f\"iter={i}; x=({x_cur[0]:.4f}, {x_cur[1]:.4f});\"\\\n",
    "          f\" f(x)={f(*x_cur):.4f}; \\\n",
    "          grad f(x)=({grad(*x_cur)[0]:.4f}, {grad(*x_cur)[1]:.4f})\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Практика_Оптимизация.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
